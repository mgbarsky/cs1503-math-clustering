\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Assignment 9: Clustering Algorithms}
\author{Your Name}

\begin{document}
\maketitle
Sometimes we perform hierarchical agglomerative clustering to discover k clusters: we can cut the dendrogram at the appropriate distance from the root to obtain a specified number of clusters.
Suppose we want to generate \textbf{two} clusters from a set of \textbf{six} tuples below:
\[ A1(7,3), A2(6,4), A3(3,6), B1(5,1), B2(9,5), B3(10,4) \]
Let the distance function be Euclidean distance.

\section*{1. K-means clustering [9 Points]} 

For each of the following pairs of initial centroids, calculate distances from each point, and assign each point to a corresponding cluster (C1 or C2). Compute SSE for each solution. 

\subsection*{1.1. First iteration [2 points]}

\subsubsection*{A. Initial centroids: A1(7,3), B1(5,1)}
\label{T11A}
\begin{table}[h]
\begin{tabular}{|l|l|l|l|}
\hline
          & Cluster 1 or 2 & A1(7,3) & B1(5, 1) \\ \hline
A1(7,3)   &                &         &          \\ \hline
A2(6, 4)  &                &         &          \\ \hline
A3(3, 6)  &                &         &          \\ \hline
B1(5, 1)  &                &         &          \\ \hline
B2(9, 5)  &                &         &          \\ \hline
B3(10, 4) &                &         &          \\ \hline
\end{tabular}
\end{table}
\label{T11A}

\subsubsection*{B. Initial centroids: A2(6,4), B2(9,5)}
\label{T11B}
\begin{table}[h]
\begin{tabular}{|l|l|l|l|}
\hline
          & Cluster 1 or 2 & A2(6, 4) & B2(9, 5) \\ \hline
A1(7,3)   &                &          &          \\ \hline
A2(6, 4)  &                &          &          \\ \hline
A3(3, 6)  &                &          &          \\ \hline
B1(5, 1)  &                &          &          \\ \hline
B2(9, 5)  &                &          &          \\ \hline
B3(10, 4) &                &          &          \\ \hline
\end{tabular}
\end{table}
\label{T11B}


\newpage
\subsection*{1.2. Second iteration [4 points]}

Compute new centroids, and recompute new distances.

\subsubsection*{A. New centroids:}
$C_1=$ \\
$C_2=$ \\
New distances:

\begin{table}[h]
\label{T12A}
\begin{tabular}{|l|l|l|l|}
\hline
          & Cluster 1 or 2 & $C_1$ & $C_2$\\ \hline
A1(7,3)   &                &  &  \\ \hline
A2(6, 4)  &                &  &  \\ \hline
A3(3, 6)  &                &  &  \\ \hline
B1(5, 1)  &                &  &  \\ \hline
B2(9, 5)  &                &  &  \\ \hline
B3(10, 4) &                &  &  \\ \hline
\end{tabular}
\end{table}
\label{T12A}

\subsubsection*{B. New centroids:}
$C_1=$ \\
$C_2=$ \\
New distances:

\begin{table}[h]
\label{T12B}
\begin{tabular}{|l|l|l|l|}
\hline
          & Cluster 1 or 2 & $C_1$ & $C_2$ \\ \hline
A1(7,3)   &                &  &  \\ \hline
A2(6, 4)  &                &  &  \\ \hline
A3(3, 6)  &                &  &  \\ \hline
B1(5, 1)  &                &  &  \\ \hline
B2(9, 5)  &                &  &  \\ \hline
B3(10, 4) &                &  &  \\ \hline
\end{tabular}
\end{table}
\label{T12B}

\subsection*{1.3. Comparing two clustering results [3 points]}
Compare the total SSE of two clusters obtained after the second iteration of K-means for A and B:
\[
SSE_A = \sum_{i=1}^{K} \sum_{x \in C_i} \left[ \text{dist}(m_i, x) \right]^2=
\]
\[
SSE_B = \sum_{i=1}^{K} \sum_{x \in C_i} \left[ \text{dist}(m_i, x) \right]^2=
\]
Explain what the difference in SSE tells us about the quality of the clusters in each case.

\newpage
\section*{2. Hierarchical clustering [10 points]} 
\subsection*{2.1. Full hierarchical clustering [6 points]}
Use the same points as above to perform full hierarchical clustering:
\[ A1(7,3), A2(6,4), A3(3,6), B1(5,1), B2(9,5), B3(10,4) \]

Fill in the original proximity matrix:

\begin{table}[h]
\label{T21}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
         & A1(7,3) & A2(6, 4) & A3(3, 6) & B1(5, 1) & B2(9, 5) & B3(10,4) \\ \hline
A1(7,3)  & 0       &          &          &          &          &          \\ \hline
A2(6, 4) &         & 0        &          &          &          &          \\ \hline
A3(3, 6) &         &          & 0        &          &          &          \\ \hline
B1(5, 1) &         &          &          & 0        &          &          \\ \hline
B2(9, 5) &         &          &          &          & 0        &          \\ \hline
B3(10,4) &         &          &          &          &          & 0        \\ \hline
\end{tabular}
\end{table}
\label{T2}

Use the \textbf{MAX} as inter-cluster distance.

Show every step and the updated proximity matrix at each step. Also draw a final dendrogram.
After that cut the dendrogram to obtain two clusters.









\subsection*{2.2. Cluster quality comparisons [4 points]}

Compute SSE of these two clusters, and compare their quality to the clusters obtained after two steps of K-means in question 1.

\[
SSE_{hierarchical} = \sum_{i=1}^{K} \sum_{x \in C_i} \left[ \text{dist}(m_i, x) \right]^2=
\]


\end{document}